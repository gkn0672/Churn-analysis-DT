{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o0VhAjpIoSZl"
      },
      "source": [
        "# CSCI316 Assignment 1 Task 2\n",
        "- Name: Nguyen Gia Khanh\n",
        "- UOWID: 7311217"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Y2Q3G_0oSZm"
      },
      "source": [
        "### Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X4_7R1auoSZn"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import math"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPb7hH8-oSZn"
      },
      "source": [
        "### Read data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o54iEJiWoSZn",
        "outputId": "f3c10de6-c155-45a3-c76c-96717abb74a0"
      },
      "outputs": [],
      "source": [
        "df1 = pd.read_csv(\"customer_churn_dataset-training-master.csv\")\n",
        "df2 = pd.read_csv(\"customer_churn_dataset-testing-master.csv\")\n",
        "df = pd.concat([df1, df2], axis=0)\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4K9gKj6HoSZo"
      },
      "source": [
        "### Clean data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Remove missing value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = df.dropna()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Data binning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DTIOPwUPoSZo",
        "outputId": "32c3177b-b80a-4f34-923f-ee6b8caf1f07"
      },
      "outputs": [],
      "source": [
        "# Total spend binning\n",
        "column_to_bin = 'Total Spend'\n",
        "bin_labels = [1, 2, 3, 4, 5]\n",
        "# Gender binning\n",
        "gender = {'Male' : 0, 'Female' : 1}\n",
        "# Subscription Type binning\n",
        "sub_type = {'Standard' : 0, 'Basic' : 1, 'Premium' : 2}\n",
        "# Contract Length binning\n",
        "cont_len = {'Annual' : 0, 'Monthly' : 1, 'Quarterly' : 2}\n",
        "# Total spend\n",
        "df['Total Spend Binned'] = pd.cut(df[column_to_bin], bins=5, labels=bin_labels)\n",
        "df.insert(loc=9, column = \"Total Spend Bin\", value = df['Total Spend Binned'])\n",
        "# Gender\n",
        "df['Gender'] = df['Gender'].map(gender)\n",
        "# Sub type\n",
        "df['Subscription Type'] = df['Subscription Type'].map(sub_type)\n",
        "# Contr len\n",
        "df['Contract Length'] = df['Contract Length'].map(cont_len)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Drop unecessary columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = df.drop(['CustomerID', 'Total Spend', 'Total Spend Binned'], axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Split test function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "# Calculate the split index\n",
        "split_index = int(0.8 * len(df))\n",
        "# Split the DataFrame\n",
        "train_df = df[:split_index]\n",
        "test_df = df[split_index:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sns.countplot(data=df, x='Churn')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize - Correlation matrix\n",
        "# Create a correlation matrix\n",
        "corr_matrix = df.corr()\n",
        "# Select the correlation values with 'default_ind'\n",
        "target_corr = corr_matrix['Churn']\n",
        "# Plot the correlation matrix as a heatmap\n",
        "plt.figure(figsize=(10, 10))\n",
        "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
        "plt.title('Correlation Matrix')\n",
        "plt.show() "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Decision tree from scratch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQL3gQB3oSZp"
      },
      "source": [
        "### Some helper function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIdy3Xq4oSZp"
      },
      "source": [
        "#### Check purity function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gAOS9PSloSZp"
      },
      "outputs": [],
      "source": [
        "# Return True if there's only one unique class in the data, False otherwise\n",
        "def CheckPurity(data):\n",
        "    class_col = data[:,-1]\n",
        "    unique_class = np.unique(class_col)\n",
        "    if len(unique_class) == 1:\n",
        "        return True\n",
        "    return False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CbYKymtNoSZp"
      },
      "source": [
        "#### Classify function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sqhm4V-FoSZp"
      },
      "outputs": [],
      "source": [
        "# Return the most frequent class in the dataset\n",
        "def Classify(data):\n",
        "    class_col = data[:,-1]\n",
        "    unique_class, count_class = np.unique(class_col, return_counts=True)\n",
        "    major_class = np.argmax(count_class)\n",
        "    return unique_class[major_class]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKJrEobkoSZp"
      },
      "source": [
        "#### Split checking function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "00ZKLLy0oSZp"
      },
      "outputs": [],
      "source": [
        "# return all possible splits for each feature\n",
        "def SplitCheck(data):\n",
        "    potential = {}\n",
        "    _, feature_num = np.shape(data)\n",
        "    for feature in range(feature_num - 1):\n",
        "        potential[feature] = []\n",
        "        value_list = np.unique(data[:, feature])\n",
        "        # For categorical values\n",
        "        if isinstance(value_list[0], str):\n",
        "            for val in value_list:\n",
        "                potential[feature].append(val)\n",
        "        else:\n",
        "            # For continuous values\n",
        "            for index in range(len(value_list)):\n",
        "                if index != 0:\n",
        "                    current_val = value_list[index]\n",
        "                    previous_val = value_list[index - 1]\n",
        "                    potential[feature].append((current_val + previous_val) / 2)\n",
        "\n",
        "    # Remove empty potential[feature] lists\n",
        "    potential = {k: v for k, v in potential.items() if v}\n",
        "    return potential"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhdA0SQVoSZq"
      },
      "source": [
        "#### Split function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TXpJK5WSoSZq"
      },
      "outputs": [],
      "source": [
        "# Split the data into two parts based on the value of a feature\n",
        "def split(data, feature, value):\n",
        "    feature_list = data[:, feature]\n",
        "    if isinstance(feature_list[0], str):\n",
        "        data_left = data[feature_list == value]\n",
        "        data_right = data[feature_list != value]\n",
        "    else:\n",
        "        data_left = data[feature_list <= value]\n",
        "        data_right = data[feature_list > value]\n",
        "    return data_left, data_right"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0SzpkOZioSZq"
      },
      "source": [
        "#### Calulate entropy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cSgzx-lUoSZq"
      },
      "outputs": [],
      "source": [
        "# Calculate the entropy of a dataset\n",
        "def Entropy(data):\n",
        "    label_col = data[:, -1]\n",
        "    _,value_count = np.unique(label_col, return_counts=True)\n",
        "    ent = 0.0\n",
        "    for i in value_count:\n",
        "        probabilities = i / sum(value_count)\n",
        "        ent += - probabilities * math.log(probabilities, 2)\n",
        "    return ent\n",
        "# Calculate the entropy of a split dataset\n",
        "def TotalEntropy(data_left, data_right):\n",
        "    total_length = len(data_left) + len(data_right)\n",
        "    probabilities_left = len(data_left) / total_length\n",
        "    probabilities_right = len(data_right) / total_length\n",
        "    total_ent = probabilities_left * Entropy(data_left) + probabilities_right * Entropy(data_right)\n",
        "    return total_ent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1wR3DmKv4JWG"
      },
      "source": [
        "#### Info Gain Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2B-Gaaw8oSZq"
      },
      "outputs": [],
      "source": [
        "# split data set based on info gain\n",
        "def InfoGainSplit(data, potential_split):\n",
        "    IGMAX = -1.0\n",
        "    split_key = -1.0\n",
        "    split_val = -1.0\n",
        "    ENT = Entropy(data)\n",
        "    for key, value_list in potential_split.items():\n",
        "        for value in value_list:\n",
        "            data_left, data_right = split(data, key, value)\n",
        "            IG = ENT - TotalEntropy(data_left, data_right)\n",
        "            if IG >= IGMAX:\n",
        "                IGMAX = IG\n",
        "                split_key = key\n",
        "                split_val = value\n",
        "    return split_key, split_val"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0yIodIHfoSZr"
      },
      "source": [
        "#### Calculate Gini index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lH-BMZLGoSZr"
      },
      "outputs": [],
      "source": [
        "# Calculate Gini index of a dataset\n",
        "def Gini(data):\n",
        "    label_col = data[:, -1]\n",
        "    _,value_count = np.unique(label_col, return_counts=True)\n",
        "    gini = 0.0\n",
        "    for i in value_count:\n",
        "        probabilities = i / sum(value_count)\n",
        "        gini += probabilities ** 2\n",
        "    return 1 - gini\n",
        "# Calculate Gini index of a split\n",
        "def TotalGini(data_left, data_right):\n",
        "    total_length = len(data_left) + len(data_right)\n",
        "    probabilities_left = len(data_left) / total_length\n",
        "    probabilities_right = len(data_right) / total_length\n",
        "    total_gini = probabilities_left * Gini(data_left) + probabilities_right * Gini(data_right)\n",
        "    return total_gini"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Gini index split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split data based on gini index\n",
        "def GiniSplit(data, potential_split):\n",
        "    GINIMIN = 9999\n",
        "    split_key = -1.0\n",
        "    split_val = -1.0\n",
        "    for key, value_list in potential_split.items():\n",
        "        for value in value_list:\n",
        "            data_left, data_right = split(data, key, value)\n",
        "            current_gini = TotalGini(data_left, data_right)\n",
        "            if current_gini <= GINIMIN:\n",
        "                GINIMIN = current_gini\n",
        "                split_key = key\n",
        "                split_val = value\n",
        "    return split_key, split_val"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Gain ratio split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split data based on gain ratio\n",
        "def GainRatioSplit(data, potential_split):\n",
        "    GAIN = -9999\n",
        "    split_key = -1.0\n",
        "    split_val = -1.0\n",
        "    ENT = Entropy(data)\n",
        "    for key, value_list in potential_split.items():\n",
        "        for value in value_list:\n",
        "            data_left, data_right = split(data, key, value)\n",
        "            splitInfo = Entropy(data_left) + Entropy(data_right)\n",
        "            IG = ENT - TotalEntropy(data_left, data_right)\n",
        "            current_gain = IG / splitInfo\n",
        "            if current_gain >= GAIN:\n",
        "                GAIN = current_gain\n",
        "                split_key = key\n",
        "                split_val = value\n",
        "    return split_key, split_val"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZR1SdOKioSZr"
      },
      "source": [
        "#### Tree induction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gxdvJ1pZoSZr"
      },
      "outputs": [],
      "source": [
        "# Build decision tree\n",
        "def DT(data, model, min_samples = 2, max_depth = 10, counter = 0):\n",
        "    # base case\n",
        "    if (CheckPurity(data)) or (len(data) < min_samples) or (counter == max_depth):\n",
        "        return Classify(data)\n",
        "    else:\n",
        "        counter+=1\n",
        "        potential_split = SplitCheck(data)\n",
        "        if not potential_split:\n",
        "            return Classify(data)\n",
        "        else:\n",
        "            if model == \"Info Gain\":\n",
        "                split_key, split_value = InfoGainSplit(data, potential_split)\n",
        "            if model == \"Gini\":\n",
        "                split_key, split_value = GiniSplit(data, potential_split)\n",
        "            if model == \"Gain Ratio\":\n",
        "                split_key, split_value = GainRatioSplit(data, potential_split)   \n",
        "            data_left, data_right = split(data, split_key, split_value)\n",
        "            if len(data_left) == 0 or len(data_right) == 0:\n",
        "                return Classify(data)\n",
        "            question = \"{} <= {}\".format(split_key, split_value)\n",
        "            subtree = {question:[]}\n",
        "            yes_ans = DT(data_left, model, min_samples, max_depth, counter)\n",
        "            no_ans = DT(data_right, model, min_samples, max_depth, counter)\n",
        "\n",
        "            if yes_ans == no_ans:\n",
        "                subtree = yes_ans\n",
        "            else:\n",
        "                subtree[question].append(yes_ans)\n",
        "                subtree[question].append(no_ans)\n",
        "    return subtree"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LfRSA4sxoSZr"
      },
      "source": [
        "#### Classification data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3SM5e7HzoSZr"
      },
      "outputs": [],
      "source": [
        "# Check if a feature is categorical or continuous\n",
        "def is_numeric_string(variable):\n",
        "    if \".\" in variable:\n",
        "        return variable.replace(\".\", \"\").isdigit()\n",
        "    return variable.isdigit()\n",
        "# Prediction\n",
        "def Classification_data(entry, tree):\n",
        "    questions = list(tree.keys())[0]\n",
        "    feature, compare_opr, value = questions.split()\n",
        "    # Categorical\n",
        "    if not is_numeric_string(value):\n",
        "        if entry[int(feature)] == value:\n",
        "            answer = tree[questions][0]\n",
        "        else:\n",
        "            answer = tree[questions][1]\n",
        "    # Continuous\n",
        "    else:\n",
        "        if float(entry[int(feature)]) <= float(value):\n",
        "            answer = tree[questions][0]\n",
        "        else:\n",
        "            answer = tree[questions][1]\n",
        "    # base case\n",
        "    if not isinstance(answer, dict):\n",
        "        return answer\n",
        "    else:\n",
        "        return Classification_data(entry, answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2-Dg3QU6SuH"
      },
      "source": [
        "# Benchmark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate the accuracy of the tree model\n",
        "def calculate_accuracy(df, tree):\n",
        "    df = df.copy()  # Create a copy of the DataFrame to avoid modifying the original\n",
        "    df[\"classification\"] = df.apply(Classification_data, axis=1, args=(tree,))\n",
        "    df[\"classification_correct\"] = df[\"classification\"] == df[\"Churn\"]\n",
        "    accuracy = df[\"classification_correct\"].mean()\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Construct DT\n",
        "- Search max_depth from range (5, 12)\n",
        "- Choose max_depth with highest accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kIxPdBIvtP6F"
      },
      "outputs": [],
      "source": [
        "tree = DT(train_df.values, 'Info Gain', max_depth=11)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tree1 = DT(train_df.values, 'Gini', max_depth=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tree2 = DT(train_df.values, 'Gain Ratio', max_depth=11)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "accuracy = calculate_accuracy(test_df, tree)\n",
        "accuracy1 = calculate_accuracy(test_df, tree1)\n",
        "accuracy2 = calculate_accuracy(test_df, tree2)\n",
        "print(\"Accuracy of DT - InfoGain: \", accuracy)\n",
        "print(\"Accuracy of DT - Gini: \", accuracy1)\n",
        "print(\"Accuracy of DT - GainRatio: \", accuracy2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Ensemble method\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ensemble classifier\n",
        "def ensemble(entry, tree1, tree2, tree3):\n",
        "    answer = []\n",
        "    answer.append(Classification_data(entry, tree1))\n",
        "    answer.append(Classification_data(entry, tree2))\n",
        "    answer.append(Classification_data(entry, tree3))\n",
        "    frequency = {}\n",
        "    max_frequency = 0\n",
        "    most_frequent_element = None\n",
        "    for element in answer:\n",
        "        # Update the frequency count for the element\n",
        "        if element in frequency:\n",
        "            frequency[element] += 1\n",
        "        else:\n",
        "            frequency[element] = 1\n",
        "        # Update the most frequent element if necessary\n",
        "        if frequency[element] > max_frequency:\n",
        "            max_frequency = frequency[element]\n",
        "            most_frequent_element = element\n",
        "    return most_frequent_element"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_accuracy_ensemble(df, tree1, tree2, tree3):\n",
        "    df = df.copy()  # Create a copy of the DataFrame to avoid modifying the original\n",
        "    df[\"classification\"] = df.apply(ensemble, axis=1, args=(tree1, tree2, tree3))\n",
        "    df[\"classification_correct\"] = df[\"classification\"] == df[\"Churn\"]\n",
        "    accuracy = df[\"classification_correct\"].mean()\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tot_accuracy = calculate_accuracy_ensemble(test_df, tree, tree1, tree2)\n",
        "tot_accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Decision Tree Accuracy:\n",
        "The accuracy scores of the individual decision tree models are as follows:\n",
        "\n",
        "- DT - InfoGain: 0.9335\n",
        "- DT - Gini: 0.9328\n",
        "- DT - GainRatio: 0.9342\n",
        "\n",
        "- Ensemble classifier Accuracy: 0.9340\n",
        "\n",
        "- Decision Tree Performance:\n",
        "The decision tree models built using different attribute selection measures (InfoGain, Gini, GainRatio) show comparable accuracy scores. The differences in accuracy between the three decision trees are relatively small, indicating that all three measures perform well on this dataset.\n",
        "\n",
        "- Ensemble classifier Performance:\n",
        "The Ensemble classifier's accuracy falls between the accuracy scores of the individual DTs. This suggests that the classifier is effectively utilizing the diversity of decisions made by the different DTs to produce a well-rounded prediction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
